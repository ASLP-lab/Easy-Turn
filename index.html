<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue
    Systems</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="TODO: title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header"
    style=" background: linear-gradient(225deg, rgba(235, 205, 121, 1.000) 0.000%, rgba(195, 205, 130, 1.000) 16.667%, rgba(151, 200, 142, 1.000) 33.333%, rgba(107, 192, 154, 1.000) 50.000%, rgba(68, 181, 163, 1.000) 66.667%, rgba(35, 168, 167, 1.000) 83.333%, rgba(13, 154, 163, 1.000) 100.000%);">

  </section>

  <section class="main-content">
    <h1 id="">
      <center>Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken
        Dialogue Systems</center>
    </h1>

    <div style="text-align:center; line-height:1.6; font-family:Arial, sans-serif;">

      <p align="center">
        Guojian Li<sup>1</sup>, Chengyou Wang<sup>1</sup>, Hongfei Xue<sup>1</sup>,
        Shuiyuan Wang<sup>1</sup>, Dehui Gao<sup>1</sup>, Zihan Zhang<sup>2</sup>,
        Yuke Lin<sup>2</sup>, Wenjie Li<sup>2</sup>, Longshuai Xiao<sup>2</sup>,
        Zhonghua Fu<sup>1</sup><sup>,â•€</sup>, Lei Xie<sup>1</sup><sup>,â•€</sup>
      </p>
      <p align="center">
        <sup>1</sup> Audio, Speech and Language Processing Group (ASLP@NPU), Northwestern Polytechnical University <br>
        <sup>2</sup> Huawei Technologies, China <br>
      </p>

    </div>


    <div style="text-align: center;">
      <table style="margin: auto; border-collapse: collapse;">
        <tr>
          <td>ğŸ¤ <a href="https://github.com/ASLP-lab/Easy-Turn">GitHub</a></td>
          <td>ğŸ¤– <a href="https://huggingface.co/ASLP-lab/Easy-Turn">Easy Turn Model</a></td>
          <td>ğŸ“‘ <a href="https://arxiv.org/abs/2509.23938">Paper</a></td>
          <td>ğŸŒ <a
              href="https://huggingface.co/collections/ASLP-lab/easy-turn-68d3ed0b294df61214428ea7">Huggingface</a></td>
        </tr>
      </table>
    </div>


    <!-- å¯¼èˆªæ ï¼šå±…ä¸­ã€è‡ªé€‚åº”å®½åº¦ã€åŠé€æ˜ + è·Ÿéšé¡µé¢ç§»åŠ¨ -->
    <div style="text-align:center; margin-top:10px; position: sticky; top: 10px; z-index: 1000;">
      <nav style="
        background: rgba(0, 0, 0, 0.5);
        padding: 12px 24px;
        display: inline-block;      /* å®½åº¦éšå†…å®¹ */
        border-radius: 10px;
        backdrop-filter: blur(6px); /* æ¯›ç»ç’ƒï¼Œæ”¯æŒçš„æµè§ˆå™¨ä¼šæ›´å¥½çœ‹ */
      ">
        <a href="#abstract" class="nav-link">Abstract</a>
        <a href="#video" class="nav-link">Promotional Videos</a>
        <!-- <a href="#pipeline" class="nav-link">Chuan-Pipeline</a> -->
        <a href="#model" class="nav-link">Easy Turn</a>
        <a href="#trainset" class="nav-link">Easy Turn Trainset</a>
        <a href="#datasample" class="nav-link">Data Samples</a>
        <a href="#experiments" class="nav-link">EXPERIMENTS</a>
      </nav>
    </div>

    <style>
      /* é“¾æ¥æ ·å¼ï¼šä¿æŒåŸå­—å·ï¼Œhover å˜è“+ä¸‹åˆ’çº¿ */
      .nav-link {
        color: white;
        margin: 0 15px;
        text-decoration: none;
        font-size: 16px;
        /* ä½ åŸæ¥çš„å¤§å° */
        transition: color 0.2s, text-decoration 0.2s;
      }

      .nav-link:hover {
        color: #66ccff;
        /* æµ…è“è‰² hover */
        text-decoration: underline;
      }
    </style>


    <h2 id="abstract" style="text-align: center;">Abstract</h2>
    <p>Full-duplex interaction is crucial for natural humanâ€“machine communication, yet remains challenging as it
      requires robust turn-taking detection to decide when the system should speak, listen, or remain silent. Existing
      solutions either rely on dedicated turn-taking models, most of which are not open-sourced. The few available ones
      are limited by their large parameter size or by supporting only a single modality, such as acoustic or linguistic.
      Alternatively, some approaches finetune LLM backbones to enable full-duplex capability, but this requires large
      amounts of full-duplex data, which remain scarce in open-source form. To address these issues, we propose <b>Easy
        Turn</b>â€”an open-source, modular turn-taking detection model that integrates acoustic and linguistic bimodal
      information to predict four dialogue turn states: complete, incomplete, backchannel, and wait, accompanied by the
      release of <b>Easy Turn trainset</b>, a 1,145-hour speech dataset designed for training turn-taking detection
      models. Compared to existing open-source models like <a
        href="https://github.com/ten-framework/ten-turn-detection">TEN Turn Detection</a> and <a
        href="https://github.com/pipecat-ai/smart-turn">Smart Turn V2</a>, our model achieves state-of-the-art
      turn-taking detection accuracy on our open-source Easy Turn testset. The data and model will be made publicly
      available on GitHub.</p>


    <h2 id="video" style="text-align:center;"> Demo: Interacting with <a href="https://github.com/ASLP-lab/OSUM">OSUM-Echat</a> with Easy-Turn runing</h2>
    <div style="display:flex; justify-content:center; gap:40px; margin-top:20px; flex-wrap:wrap;">
      <div style="text-align:center; max-width:600px; width:100%;">
        <!-- <h3>Sichuanese</h3> -->
        <div style="position:relative; width:100%; max-width:600px; aspect-ratio:1/1; background:#000;">
          <iframe width="100%" height="100%" src="demo_video_new_new.mp4" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen style="position:absolute; inset:0;"></iframe>
        </div>
      </div>
    </div>


    <h2 id="model" style="text-align: center;">Easy Turn</h2>
    <p>To accelerate research in full-duplex spoken dialogue field, we introduce Easy Turn, an open-source and modular
      turn-taking detection model. The model accepts user's speech as input and outputs both the corresponding ASR
      transcription and the dialogue turn state, effectively integrating acoustic and linguistic information. The Easy
      Turn comprises three key components: an audio encoder, an audio adaptor, and an LLM. Its design draws
      inspiration
      from Qwen-Audio, employing Whisper as the audio encoder and Qwen2.5 as the LLM. To better integrate acoustic and
      linguistic modalities, we adopt an ASR+Turn-Detection paradigm, where the LLM generates ASR transcriptions and
      fuses them with acoustic features to sequentially predict dialogue turn state labels (complete, incomplete,
      backchannel or wait).</p>

    <div style="text-align:center;">
      <img src="model.png" alt="model" width="600">
    </div>

    <h2 id="trainset" style="text-align: center;">Easy Turn Trainset</h2>
    <p>The Easy Turn trainset is a large-scale speech dataset designed for training turn-taking detection models,
      comprising both real and synthetic data. It comprises four dialogue turn states (complete, incomplete,
      backchannel, wait), totaling approximately 1,145 hours. Detailed statistical data are shown in the table below.
      The detailed data processing pipeline is shown in the following figure. For more details, please refer to the
      original paper.</p>
    <div style="display: flex; justify-content: space-between; text-align: center; height: 400px;">
      <div style="display: flex; align-items: center;">
        <img src="data_pipeline.jpg" alt="pipeline" width="800">
      </div>
      <div style="display: flex; align-items: center;">
        <img src="trainset.png" alt="trainset" width="800">
      </div>
    </div>

    <h2 id="datasample" style="text-align: center;">Data Samples</h2>
    <table style="text-align: center; width: 100%; border-collapse: collapse;">
      <thead>
        <tr>
          <th style="width: 10%; ">State</th>
          <th style="width: 45%; ">Real</th>
          <th style="width: 45%; ">Synthetic</th>
        </tr>
      </thead>
      <tbody>

        <tr>
          <td>Complete</td>
          <td style="text-align: center;vertical-align: middle;">

            <audio controls="" src="samples/complete_real_1.wav"></audio>
            <br>
            ä½ æœ‰æ²¡æœ‰å‘ç”Ÿè¿‡ä¸€äº›ç«¥å¹´è¶£äº‹å•Š&lt;complete&gt;
            <hr>
            <audio controls="" src="samples/complete_real_2.wav"></audio>
            <br>
            æˆ‘è§‰å¾—ç”»ç”»å¤ªéš¾äº†&lt;complete&gt;
          </td>
          <td>
            <audio controls="" src="samples/complete_synthetic_1.wav"></audio>
            <br>
            è¯·æè¿°å¤åŸƒåŠé‡‘å­—å¡”çš„è®¾è®¡ä¸ç”¨é€”&lt;complete&gt;
            <hr>
            <audio controls="" src="samples/complete_synthetic_2.wav"></audio>
            <br>
            åˆä¸­å­¦ç”Ÿçš„è¯­æ–‡å­¦ä¹ æœ‰ä»€ä¹ˆç‰¹ç‚¹&lt;complete&gt;
          </td>
        </tr>

        <tr>
          <td>Incomplete</td>
          <td style="text-align: center;vertical-align: middle;">

            <audio controls="" src="samples/incomplete_real_1.wav"></audio>
            <br>
            å› ä¸ºå°æ—¶å€™&lt;incomplete&gt;
            <hr>
            <audio controls="" src="samples/incomplete_real_2.wav"></audio>
            <br>
            æˆ‘é‚£ä¼šå„¿å°±ä¼šåœ¨å—¯&lt;incomplete&gt;
          </td>
          <td>
            <audio controls="" src="samples/incomplete_synthetic_1.wav"></audio>
            <br>
            æˆ‘æœ¬æ¥æƒ³&lt;incomplete&gt;
            <hr>
            <audio controls="" src="samples/incomplete_synthetic_2.wav"></audio>
            <br>
            å…¶å®ä¸»è¦æƒ³é—®å°±æ˜¯&lt;incomplete&gt;
          </td>
        </tr>

        <tr>
          <td>Backchannel</td>
          <td style="text-align: center;vertical-align: middle;">

            <audio controls="" src="samples/backchannel_real_1.wav"></audio>
            <br>
            å—¯å¯¹å¯¹å¯¹&lt;backchannel&gt;
            <hr>
            <audio controls="" src="samples/backchannel_real_2.wav"></audio>
            <br>
            å•Šæ˜¯å•Š&lt;backchannel&gt;
          </td>
          <td>
            <audio controls="" src="samples/backchannel_synthetic_1.wav"></audio>
            <br>
            å“¦å¯¹&lt;backchannel&gt;
            <hr>
            <audio controls="" src="samples/backchannel_synthetic_2.wav"></audio>
            <br>
            æŒºå¥½çš„&lt;backchannel&gt;
          </td>
        </tr>

        <tr>
          <td>Wait</td>
          <td style="text-align: center;vertical-align: middle;">

            <audio controls="" src="samples/wait_real_1.wav"></audio>
            <br>
            å¤ªåµäº†é©¬ä¸Šåœä¸‹&lt;wait&gt;
            <hr>
            <audio controls="" src="samples/wait_real_2.wav"></audio>
            <br>
            ç«‹å³é™éŸ³&lt;wait&gt;
          </td>
          <td>
            <audio controls="" src="samples/wait_synthetic_1.wav"></audio>
            <br>
            æˆ‘æœ‰æ€¥äº‹è¦å¤„ç†å…ˆåœä¸‹&lt;wait&gt;
            <hr>
            <audio controls="" src="samples/wait_synthetic_2.wav"></audio>
            <br>
            çœŸå—ä¸äº†ä½ åˆ«è¯´äº†&lt;wait&gt;
          </td>
        </tr>

      </tbody>
    </table>

    <h2 id="experiments" style="text-align: center;">EXPERIMENTS</h2>
    <h3 id="results" style="text-align: center;">Main Results</h3>
    <p>We evaluate Easy Turn against two open-source turn-taking detection models, TEN Turn Detection and Smart Turn
      V2,
      using the Easy Turn testset. All experiments are conducted on a single NVIDIA RTX 4090 GPU. Notably, since TEN
      Turn Detection lacks direct speech support, we use Paraformer as the ASR model to transcribe speech into text
      and
      take the text as its input. The table below reports the results: ACC_cp, ACC_incp, ACC_bc and ACC_wait denote
      the
      turn-taking detection accuracy for complete, incomplete, backchannel, and wait states (higher is better).
      Params,
      Latency, and Memory represent total model size, average inference time, and GPU usage, where lower values
      indicate
      greater efficiency. The symbol â€œâ€“â€ indicates that the corresponding model
      does not support detection of a particular state.</p>

    <table border="1" cellpadding="5" cellspacing="0" style="width: 100%;">
      <thead>
        <tr>
          <th><strong>Model</strong></th>
          <th><strong>Params(MB)</strong> &darr;</th>
          <th><strong>Latency(ms)</strong></th>
          <th><strong>Memory(MB)</strong></th>
          <th><strong>ACC<sub>cp</sub>(%)</strong> &uarr;</th>
          <th><strong>ACC<sub>incp</sub>(%)</strong></th>
          <th><strong>ACC<sub>bc</sub>(%)</strong></th>
          <th><strong>ACC<sub>wait</sub>(%)</strong></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><a href="https://github.com/modelscope/FunASR">Paraformer</a> + <a href="https://github.com/ten-framework/ten-turn-detection">TEN Turn Detection</a></td>
          <td>7220</td>
          <td>204</td>
          <td>15419</td>
          <td>86.67</td>
          <td>89.3</td>
          <td>-</td>
          <td>91</td>
        </tr>
        <tr>
          <td><a href="https://github.com/pipecat-ai/smart-turn">Smart Turn V2</a></td>
          <td><strong>95</strong></td>
          <td><strong>27</strong></td>
          <td><strong>370</strong></td>
          <td>78.67</td>
          <td>62</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td><strong>Easy Turn (Proposed)</strong></td>
          <td>850</td>
          <td>263</td>
          <td>2559</td>
          <td><strong>96.33</strong></td>
          <td><strong>97.67</strong></td>
          <td><strong>91</strong></td>
          <td><strong>98</strong></td>
        </tr>
      </tbody>
    </table>

    <h3 id="ablation" style="text-align: center;">Ablation Study</h3>
    <P>We conduct ablation experiments on the Easy Turn to evaluate the contribution of individual modalities within its
      architecture and to assess the impact of the ASR + Turn-Detection paradigm on performance. The primary metric is
      ACC_avg, computed as the average detection accuracy across four dialogue turn states
      (complete, incomplete, backchannel, and wait). Easy
      Turn only-state uses the same architecture as Easy Turn but omits the ASR + Turn-Detection
      paradigm, directly predicting dialogue turn state labels without first generating ASR transcriptions. Finetuned
      Whisper + Linear refers to fine-tuning Whisper-Medium audio encoder with an additional linear classifier on our
      Easy Turn trainset, taking only speech as input and directly predicting dialogue turn state labels, representing
      the acoustic-only modality. Finetuned Qwen2.5-0.5B-Instruct is fine-tuned on text transcriptions from our Easy
      Turn trainset, taking only text as input and outputting dialogue turn state labels, representing the
      linguistic-only modality. Detailed results are shown in the table below.</P>
    <table border="1" cellpadding="10">
      <thead>
        <tr>
          <th>Model</th>
          <th>Modality</th>
          <th><b>ACC_avg â†‘</b></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><b>Easy Turn (Proposed)</b></td>
          <td>Acoustic + Linguistic</td>
          <td><b>95.75</b></td>
        </tr>
        <tr>
          <td>Easy Turn<sub>only-state</sub></td>
          <td>Acoustic + Linguistic</td>
          <td>87.88</td>
        </tr>
        <tr>
          <td>Finetuned Qwen2.5-0.5B-Instruct</td>
          <td>Linguistic-only</td>
          <td>86.25</td>
        </tr>
        <tr>
          <td>Finetuned Whisper + Linear</td>
          <td>Acoustic-only</td>
          <td>85.50</td>
        </tr>
      </tbody>
    </table>








    <!-- <h3 id="examples" style="text-align: center;">Examples<a name="dataset"></a></h3>
    <p>We present several examples of Easy Turn applications in spoken dialogue systems. The content inside the angle
      brackets indicates the dialogue turn state detected by Easy Turn, while the text in parentheses represents the
      actions the system should take based on the detected dialogue turn state. To evaluate its performance in
      turn-taking detection, we deploy Easy Turn in our laboratory spoken dialogue system OSUM-EChat, where human users
      interact with the system through microphone input. The results show that Easy Turn performs effectively,
      accurately identifying dialogue turn states and enabling the system to respond appropriately. For the actual
      effect demonstration, you can refer to our demo page.</p> -->


  </section>
</body>

<script>
  // é…ç½®
  const systems = ["ref", "Cosyvoice2", "CosyVoice2-Chuan", "StepAudio", "Llasa-1B-Chuan"];
  const systemDisplayNames = {
    "ref": "Reference",
    "Llasa-1B-Chuan": "Llasa-1B-Chuan",
    "Cosyvoice2": "Cosyvoice2",
    "StepAudio": "Step-Audio-TTS-3B",
    "CosyVoice2-Chuan": "CosyVoice2-Chuan",
  };

  // åŠ¨æ€ç”Ÿæˆè¡¨å¤´
  const headerRow = document.getElementById('header-row');
  systems.forEach(sys => {
    const th = document.createElement('th');
    th.textContent = systemDisplayNames[sys] || sys;
    if (sys === "Llasa-1B-Chuan" || sys === "CosyVoice2-Chuan") {
      th.style.backgroundColor = "#ffe0b2"; // é«˜äº®é¢œè‰²
      th.style.fontWeight = "bold";
    }
    headerRow.appendChild(th);
  });

  // è¯»å–å¹¶è§£ætext.txt
  fetch('raw/TTS_samples/text.txt')
    .then(response => response.text())
    .then(text => {
      const lines = text.trim().split('\n');
      const data = lines.map(line => {
        const [filename, content] = line.split('|');
        return { filename: filename.trim(), text: content.trim() };
      });

      // ç”Ÿæˆè¡¨æ ¼å†…å®¹
      const tbody = document.getElementById('tbody');
      tbody.innerHTML = ""; // æ¸…ç©ºLoading
      data.forEach(item => {
        const tr = document.createElement('tr');
        // æ–‡æœ¬åˆ—
        const tdText = document.createElement('td');
        tdText.className = "text-cell";
        tdText.textContent = item.text;
        tr.appendChild(tdText);

        // å„ç³»ç»ŸéŸ³é¢‘
        systems.forEach(sys => {
          const td = document.createElement('td');
          if (sys === "Llasa-1B-Chuan" || sys === "CosyVoice2-Chuan") {
            td.style.backgroundColor = "#fff3e0"; // é«˜äº®é¢œè‰²
          }
          const audio = document.createElement('audio');
          audio.controls = true;
          audio.style.width = "180px";
          audio.src = `raw/TTS_samples/${sys}/${item.filename}.wav`;
          td.appendChild(audio);
          tr.appendChild(td);
        });
        tbody.appendChild(tr);
      });
    })
    .catch(err => {
      document.getElementById('tbody').innerHTML = `<tr><td colspan="${systems.length + 1}">Failed to load text.txt: ${err}</td></tr>`;
    });

</script>






